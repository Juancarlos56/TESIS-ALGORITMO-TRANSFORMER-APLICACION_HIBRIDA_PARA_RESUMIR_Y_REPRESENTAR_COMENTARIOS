{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d36c1a0",
   "metadata": {},
   "source": [
    "##  Fase3. Prediccion para nuevos samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f339e3",
   "metadata": {},
   "source": [
    "### Input's \n",
    "- Modelos de red Transformer\n",
    "- pickle de transformador "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771bb8f",
   "metadata": {},
   "source": [
    "Instalar Pytorch desde Conda Promt\n",
    "- conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6957fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet transformers==4.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf1274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e934ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e639429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install  --quiet nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "672150cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "477f83b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0af96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35e980f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe252949",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a42848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Juan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "###Librerias de transformador\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from Transformador.transformadorPreprocesamiento import Preprocesamiento\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667131a",
   "metadata": {},
   "source": [
    "### Carga de Transformer con Tourch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b89c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizationPredict:\n",
    "    \"\"\" Clase TextSummarizationPredict \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Inicia la clase TextSummarizationT5 \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_model(\n",
    "      self, model_type: str = \"t5\", model_dir: str = \"outputs\", use_gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "          carga un punto de control para inferencia/predicción\n",
    "          Args:\n",
    "              model_type (str, optional): \"t5\".\n",
    "              model_dir (str, optional): ruta al directorio del modelo. Default \"outputs\".\n",
    "              use_gpu (bool, optional): if True, el modelo usa gpu para inferencia/predicción. \n",
    "                                        Default True.\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            else:\n",
    "                raise \"exception ---> no gpu found. set use_gpu=False, to use CPU\"\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            source_text: str,\n",
    "            max_length: int = 32,\n",
    "            num_return_sequences: int = 1,\n",
    "            num_beams: int = 2,\n",
    "            top_k: int = 50,\n",
    "            top_p: float = 0.95,\n",
    "            do_sample: bool = True,\n",
    "            repetition_penalty: float = 2.5,\n",
    "            length_penalty: float = 1.0,\n",
    "            early_stopping: bool = True,\n",
    "            skip_special_tokens: bool = True,\n",
    "            clean_up_tokenization_spaces: bool = True,\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "            Genera predicción para el modelo T5\n",
    "            Args:\n",
    "              source_text (str): texto para generar predicción\n",
    "              max_length (int, optional): longitud máxima del token de predicción.\n",
    "              num_return_sequences (int, optional): número de predicciones a devolver. \n",
    "                                                    Default 1.\n",
    "              num_beams (int, optional): numero de beams. Default 2.\n",
    "              top_k (int, optional): Default 50.\n",
    "              top_p (float, optional): Default 0.95.\n",
    "              do_sample (bool, optional): Default True.\n",
    "              repetition_penalty (float, optional): Defaults 2.5.\n",
    "              length_penalty (float, optional): Defaults 1.0.\n",
    "              early_stopping (bool, optional): Defaults True.\n",
    "              skip_special_tokens (bool, optional): Defaults True.\n",
    "              clean_up_tokenization_spaces (bool, optional): Defaults True.\n",
    "            Returns:\n",
    "              list[str]: returns predicción\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(source_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        generated_ids = self.model.generate(\n",
    "          input_ids=input_ids,\n",
    "          num_beams=num_beams,\n",
    "          max_length=max_length,\n",
    "          repetition_penalty=repetition_penalty,\n",
    "          length_penalty=length_penalty,\n",
    "          early_stopping=early_stopping,\n",
    "          top_p=top_p,\n",
    "          top_k=top_k,\n",
    "          num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "        preds = [\n",
    "            self.tokenizer.decode(\n",
    "              g,\n",
    "              skip_special_tokens=skip_special_tokens,\n",
    "              clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            )\n",
    "            for g in generated_ids\n",
    "        ]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6968d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextSummarizationPredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6dffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained T5 model\n",
    "model.load_model(\"t5\",\"modelo/TextSummarizationT5/TextSummarizationT5-epoch-7-train-loss-0.4759-val-loss-1.2537\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2525cf6",
   "metadata": {},
   "source": [
    "### Preparación "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd408e",
   "metadata": {},
   "source": [
    "#### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c5cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargarPipeline(nombreArchivo):\n",
    "    with open(nombreArchivo+'.pickle', 'rb') as handle:\n",
    "        pipeline = pickle.load(handle)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67ee4fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('texto_preprocesado', Preprocesamiento())])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hacia tiempo que no comiamos algo tan bueno y ...</td>\n",
       "      <td>hacia tiempo que no comiamos algo tan bueno y ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  Hacia tiempo que no comiamos algo tan bueno y ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  hacia tiempo que no comiamos algo tan bueno y ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Transformador.transformadorPreprocesamiento import Preprocesamiento\n",
    "pipe = cargarPipeline('Transformador/pipePreprocesador')\n",
    "print(pipe)\n",
    "text = 'Hacia tiempo que no comiamos algo tan bueno y de tanta calidad. Si es cierto que el sitio se hace un poco pequeño y dias de fin de semana seguramente se llene mucho. Pero la comida es excelente, cada plato que pruebas, hace que quieras probar otro que saca a otros comensales. Un sitio muy recomendado.'\n",
    "textProcesado = pipe.transform(text)\n",
    "textProcesado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1501b8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hacia tiempo que no comiamos algo tan bueno y de tanta calidad  si es cierto que el sitio se hace un poco pequeño y dias de fin de semana seguramente se llene mucho  pero la comida es excelente  cada plato que pruebas  hace que quieras probar otro que saca a otros comensales  un sitio muy recomendado '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textProcesado['cleaned_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9726e60",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01044f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comida es excelente']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_summarize=textProcesado['cleaned_text'][0]\n",
    "model.predict(text_to_summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829f74d",
   "metadata": {},
   "source": [
    "### Clase para Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a3efa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizationPredict:\n",
    "    \"\"\" Clase TextSummarizationPredict \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Inicia la clase TextSummarizationT5 \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_model(\n",
    "      self, model_type: str = \"t5\", model_dir: str = \"outputs\", use_gpu: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "          carga un punto de control para inferencia/predicción\n",
    "          Args:\n",
    "              model_type (str, optional): \"t5\".\n",
    "              model_dir (str, optional): ruta al directorio del modelo. Default \"outputs\".\n",
    "              use_gpu (bool, optional): if True, el modelo usa gpu para inferencia/predicción. \n",
    "                                        Default True.\n",
    "        \"\"\"\n",
    "        if model_type == \"t5\":\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(f\"{model_dir}\")\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(f\"{model_dir}\")\n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = torch.device(\"cuda\")\n",
    "            else:\n",
    "                raise \"exception ---> no gpu found. set use_gpu=False, to use CPU\"\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "    def predict(\n",
    "            self,\n",
    "            source_text: str,\n",
    "            max_length: int = 32,\n",
    "            num_return_sequences: int = 1,\n",
    "            num_beams: int = 2,\n",
    "            top_k: int = 50,\n",
    "            top_p: float = 0.95,\n",
    "            do_sample: bool = True,\n",
    "            repetition_penalty: float = 2.5,\n",
    "            length_penalty: float = 1.0,\n",
    "            early_stopping: bool = True,\n",
    "            skip_special_tokens: bool = True,\n",
    "            clean_up_tokenization_spaces: bool = True,\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "            Genera predicción para el modelo T5\n",
    "            Args:\n",
    "              source_text (str): texto para generar predicción\n",
    "              max_length (int, optional): longitud máxima del token de predicción.\n",
    "              num_return_sequences (int, optional): número de predicciones a devolver. \n",
    "                                                    Default 1.\n",
    "              num_beams (int, optional): numero de beams. Default 2.\n",
    "              top_k (int, optional): Default 50.\n",
    "              top_p (float, optional): Default 0.95.\n",
    "              do_sample (bool, optional): Default True.\n",
    "              repetition_penalty (float, optional): Defaults 2.5.\n",
    "              length_penalty (float, optional): Defaults 1.0.\n",
    "              early_stopping (bool, optional): Defaults True.\n",
    "              skip_special_tokens (bool, optional): Defaults True.\n",
    "              clean_up_tokenization_spaces (bool, optional): Defaults True.\n",
    "            Returns:\n",
    "              list[str]: returns predicción\n",
    "        \"\"\"\n",
    "        input_ids = self.tokenizer.encode(source_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        input_ids = input_ids.to(self.device)\n",
    "        generated_ids = self.model.generate(\n",
    "          input_ids=input_ids,\n",
    "          num_beams=num_beams,\n",
    "          max_length=max_length,\n",
    "          repetition_penalty=repetition_penalty,\n",
    "          length_penalty=length_penalty,\n",
    "          early_stopping=early_stopping,\n",
    "          top_p=top_p,\n",
    "          top_k=top_k,\n",
    "          num_return_sequences=num_return_sequences,\n",
    "        )\n",
    "        preds = [\n",
    "            self.tokenizer.decode(\n",
    "              g,\n",
    "              skip_special_tokens=skip_special_tokens,\n",
    "              clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
    "            )\n",
    "            for g in generated_ids\n",
    "        ]\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd4174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "###Librerias de transformador\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast as T5Tokenizer,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from Transformador.transformadorPreprocesamiento import Preprocesamiento\n",
    "import tqdm as notebook_tqdm\n",
    "from Transformador.transformadorPreprocesamiento import Preprocesamiento\n",
    "\n",
    "class SummarizationPredict:\n",
    "    \"\"\" Clase TextSummarizationPredict \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        \"\"\" Inicia la clase TextSummarizationT5 \"\"\"\n",
    "        pass\n",
    "   \n",
    "    def cargarPipeline(self, nombreArchivo):\n",
    "        with open(nombreArchivo+'.pickle', 'rb') as handle:\n",
    "            pipeline = pickle.load(handle)\n",
    "        return pipeline\n",
    "    \n",
    "    def limpiezaTextoParaPredict(self, texto):\n",
    "        pipe = self.cargarPipeline('Transformador/pipePreprocesador')\n",
    "        text = texto\n",
    "        textProcesado = pipe.transform(text)\n",
    "        return textProcesado['cleaned_text'][0]\n",
    "    \n",
    "    def predictSummarization(self, texto):\n",
    "        model = TextSummarizationPredict()\n",
    "        model.load_model(\"t5\",\"modelo/TextSummarizationT5/TextSummarizationT5-epoch-7-train-loss-0.4759-val-loss-1.2537\", use_gpu=False)\n",
    "        text_to_summarize=self.limpiezaTextoParaPredict(texto)\n",
    "        return(model.predict(text_to_summarize))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023831ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comida es excelente']\n"
     ]
    }
   ],
   "source": [
    "texto = 'Hacia tiempo que no comiamos algo tan bueno y de tanta calidad. Si es cierto que el sitio se hace un poco pequeño y dias de fin de semana seguramente se llene mucho. Pero la comida es excelente, cada plato que pruebas, hace que quieras probar otro que saca a otros comensales. Un sitio muy recomendado.'\n",
    "model = SummarizationPredict().predictSummarization(texto)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eccfba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['servicio y comida muy buenos']\n"
     ]
    }
   ],
   "source": [
    "texto = 'El servicio y la comida son muy buenos en este restaurante. Tienen un bar completo y un menú caro con delicias españolas. Dicho esto, debo advertir al lector las porciones no son abundantes y es caro. Fuimos allí para comer el mes pasado y realmente disfrutamos de la paella.'\n",
    "model = SummarizationPredict().predictSummarization(texto)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702bba8",
   "metadata": {},
   "source": [
    "### Probando Modelo con nuevos comentarios \n",
    "\n",
    "- Los comentarios fueron tomados de: https://www.tripadvisor.com/Restaurant_Review-g294309-d1928308-Reviews-Restaurante_El_Jardin-Cuenca_Azuay_Province.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "521ef1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Juan\\anaconda3\\envs\\Tesis-Env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comentario</th>\n",
       "      <th>resumen generado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Un lugar muy bonito, con mucha decoración, agr...</td>\n",
       "      <td>atención agradable ambiente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La comida deliciosa! El ambiente hermoso, la a...</td>\n",
       "      <td>ambiente hermoso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El ambiente de lujo, la atención esmerada y co...</td>\n",
       "      <td>ambiente de lujo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El restaurante El Jardin del Hotel Victoria, e...</td>\n",
       "      <td>restaurante servicio bueno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llegamos con muy altas expectativas.A las 13h4...</td>\n",
       "      <td>comida fue muy decepcionante</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          comentario  \\\n",
       "0  Un lugar muy bonito, con mucha decoración, agr...   \n",
       "1  La comida deliciosa! El ambiente hermoso, la a...   \n",
       "2  El ambiente de lujo, la atención esmerada y co...   \n",
       "3  El restaurante El Jardin del Hotel Victoria, e...   \n",
       "4  Llegamos con muy altas expectativas.A las 13h4...   \n",
       "\n",
       "               resumen generado  \n",
       "0   atención agradable ambiente  \n",
       "1              ambiente hermoso  \n",
       "2              ambiente de lujo  \n",
       "3    restaurante servicio bueno  \n",
       "4  comida fue muy decepcionante  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comentarios = ['Un lugar muy bonito, con mucha decoración, agradable ambiente. La atención es muy demorada y la comida no es lo que se espera.',\n",
    "               'La comida deliciosa! El ambiente hermoso, la atención excelente, y todo por precios razonables! Definitivamente recomendado!', \n",
    "               'El ambiente de lujo, la atención esmerada y con mucha amabilidad, la comida con una combinación de sencillez y que resalta cada sabor en la boca. No soy de comer postres, pero me dieron uno de crema de menta que es un real éxtasis.' ,\n",
    "              'El restaurante El Jardin del Hotel Victoria, es un lugar que conserva una tradición de más de 40 años en Cuenca, pues este fue el primer restaurante de servicio completo que abrió en la ciudad y que se mantiene hasta la actualidad. Una carta bien lograda, con platos esenciales y bien servidos siguiendo principios culinarios de siempre. El servicio es muy bueno, y basta haber ido más d una vez que la relación con sus dueños y empleados se vuelve absolutamente familiar. La vista del Tomebamba y del jardín del Hotel representan un valor agregado enorme para las personas que buscan un remanso pacífico en sus labores. Muy recomendables las sugerencias diarias del chef y las propuestas gastronómicas de temporada. Actualmente se encuentra solo con servicio a domicilio, esperado que vuelva pronto a abrir sus puertas según comentan sus propietarios.',\n",
    "              'Llegamos con muy altas expectativas.A las 13h45 ingresamos, nos tomaron el pedido a las 14h00, las entradas las trajeron a las 14h40, luego de eso los platos fuertes llegaron a la 15h25, y cuando llegó la comida fue muy decepcionante, platos desabridos que ni siquiera entran dentro de un gusto normal... finalmente las bebidas nunca llegaron. Uno de los meseros a mi parecer dio un servicio quemimportista.Sentí que boté el dinero a la basura.No volveré a este lugar, lástima que la vista es bonita.']\n",
    "\n",
    "values = []\n",
    "for comentario in comentarios:\n",
    "    summary = SummarizationPredict().predictSummarization(comentario)\n",
    "    values.append([comentario, summary[0]])\n",
    "\n",
    "column_name = ['comentario','resumen generado']               \n",
    "dfResultados = pd.DataFrame(values,columns=column_name)\n",
    "dfResultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f0ce2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf025348",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResultados.to_excel(r'TablaResultadosResumenesPredict.xlsx', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
